#!/usr/bin/env python
# coding: utf-8

# ***Проект: Обучение с учителем: качество модели***

# In[1]:


##Импортируем необходимые библиотеки
get_ipython().system('pip install scikit-learn==1.1.3 ')
get_ipython().system('pip install shap -q ')
get_ipython().system('pip install phik -q')

import pandas as pd
import matplotlib.pyplot as plt 
import seaborn as sns
import math
import numpy as np
import phik
import shap
import statsmodels.api as sm


from sklearn.pipeline import Pipeline
from sklearn.inspection import permutation_importance
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.metrics import roc_auc_score, f1_score
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant
from sklearn.preprocessing import ( StandardScaler, 
                                  OneHotEncoder, 
                                  OrdinalEncoder,
                                  MinMaxScaler,
                                  LabelEncoder)#добавлена для кодирования по комментарию

get_ipython().system('pip install -U scikit-learn #добавлена по комментарию')


# In[2]:


#Сразу зададим константы для будущей модели
RANDOM_STATE = 46
TEST_SIZE = 0.25


# **Шаг 1. Загрузка данных**

# In[3]:


##Загрузим данные:
market_file =pd.read_csv('/datasets/market_file.csv')
market_money=pd.read_csv('/datasets/market_money.csv')
market_time=pd.read_csv('/datasets/market_time.csv')
money=pd.read_csv('/datasets/money.csv',sep=';',decimal=",")


# **Шаг.1.1 - изучение данных**

# In[4]:


#Напишим функцию для оптимизации повторяющихся действий
#Функция info_data
def info_data(data):
    print('Изучим топ 5 строк DF')
    display(data.head())
    print('')
    print('Изучим Общую информацию DF')
    display(data.info())
    print('')
    print('Изучим описательную статистику DF')
    display(data.describe().T)
    print('')
    print('Посчитаем кол-во пропусков')
    print('')
    print('Колличество пропусков:\n', data.isna().sum())
    print('Посчитаем кол-во дубликатов')
    print('')
    print('Колличество дубликатов:\n',data.duplicated().sum())


# *Изучим market_file*

# In[5]:


info_data(market_file)


# *Вывод по таблице "market_file"*
# 
#     -Явных пропусков  и дубликатов не обнаружино
#     -Типы данных соответствуют описанию таблицы
#     -в ряде названий колонок есть пробелы, которые обработае в блоке "предобработка данных"

# *Изучим market_money*

# In[6]:


info_data(market_money)


# Вывод по таблице "market_money" 
# 
#     -Явных пропусков  и дубликатов не обнаружино
#     -Типы данных соответствуют описанию таблицы

# *Изучим market_time*

# In[7]:


info_data(market_time)


# Вывод по таблице "market_time" 
# 
#     -Явных пропусков  и дубликатов не обнаружино
#     -Типы данных соответствуют описанию таблицы

# *Изучим money*

# In[8]:


info_data(money)


# Вывод по таблице "market" 
# 
#     -Явных пропусков  и дубликатов не обнаружино
#     -Типы данных соответствуют описанию таблицы

# *Общий вывод по блоку*
# 
#      -Все необходимые данные загружены
#      -Явных пропусков и дубликатов не обнаружено
#      -В таблице market_file есть проблемы в названиях столбцов, поправим их в сл разделе

# ***Шаг 2. Предобработка данных***

# *Уберем пробелы в названиях столбцов в таблице market_file*

# In[9]:


def columns_remove_spaces(df):
    df.columns = df.columns.str.replace(' ', '_').str.lower()
    


# In[10]:


columns_remove_spaces(market_file)
columns_remove_spaces(market_money)
columns_remove_spaces(market_time)
columns_remove_spaces(money)


# In[11]:


#проверяем изменения
market_file.head()


# Общий вывод по блоку
# 
#   - Убрали пробелы в названия столбцов market_file
#   - Иная преобработка данныз не нужна т к в изучении данных в первом блоке никаких отклонений не обнаружено

# ***Шаг 3. Исследовательский анализ данных***

# В данном шаге сделаем проверку данных на:
# 
#     - Выбросы
#     - Аномальные данные
#     - Прочии анамалии / ошибки в данных

# Создадим функции для построения график и оптимизации повторяющихся процессов 

# In[12]:


#Содание функции
#функция для построения гистограммы
def histogram(df, col, target):
    plt.figure(figsize=(8,6))
    plot = sns.histplot(df, bins=20, kde=True, hue=target, x=col)
    plot.set_title(f'Рапределение по {col}', fontsize=16)
    plot.set_ylabel('Количество', fontsize=14)
    

#функция для подсчета уникальных записей, кол-ва записей и построения Круговой диаграммы
def categoral_unique(df, col):
    display(df[col].unique())
    value_counts = df[col].value_counts()
    plt.figure(figsize=(8,8))
    plt.pie(value_counts, labels=value_counts.index, autopct='%1.1f%%', startangle=140)
    plt.title(f'Распределение по {col}', fontsize=16)
    plt.show()
    
def hist_with_wiskers(df, col, target):
    sns.set()
    f, axes = plt.subplots(1, 2, figsize=(16, 4))
    axes[0].set_title(f'Распределение признака  {col}', fontsize=16)
    axes[0].set_ylabel('Плотность распределения', fontsize=14)
    if target != None:
        sns.histplot(df, stat='density', common_norm=False ,bins=20, kde=True, ax=axes[0], hue=target, x=col)
    else:
        sns.histplot(df, stat='density', common_norm=False ,bins=20, kde=True, ax=axes[0], hue=target,x=col)
    axes[1].set_title(f'График ящик с усами для признака {col}', fontsize=16)
    sns.boxplot(data=df, ax=axes[1], y=col)
    axes[1].set_ylabel(col, fontsize=14)
    plt.show()

def pivot_pie_chart(df, col):
    plt.figure(figsize=(8,8))
    plt.pie(df[col].value_counts(), labels=df[col].value_counts().index, autopct='%1.1f%%', startangle=140)
    plt.title(f'Распределение по {col}', fontsize=16)
    plt.show()


# *Исследуем market_file*

# In[13]:


#Иследуем Покупательскую_активность
categoral_unique(market_file, 'покупательская_активность')


# *Наблюдаетя дисбаланс покупательской активности*

# In[14]:


#Исследуем Тип_сервиса
categoral_unique(market_file, 'тип_сервиса')


# *Выявилась опечатка данных нужно заменить стандартт на стандарт*
# *Также можно наблюдать, что клиентов премиум сильно меньше, чем стандартных*

# In[15]:


#Заменяем опечатку в "стандартт" на "стандарт"
market_file['тип_сервиса']=market_file['тип_сервиса'].replace('стандартт','стандарт')


# In[16]:


#Проверяем
market_file['тип_сервиса'].unique()


# In[17]:


#исследуем разрешить_сообщать
categoral_unique(market_file, 'разрешить_сообщать')


# *Преобладает да, что логично*

# In[18]:


#Проанализиуем с помощью яшика с усами и графика распределения  "Маркет_актив_6_мес" и  "Покупательская_активность"
hist_with_wiskers(market_file, 'маркет_актив_6_мес','покупательская_активность')


# **По распределению и ящика  усами видны выбросы <2, посмотрим на них подробнее ниже**

# In[19]:


market_file.query('маркет_актив_6_мес<2')


# *Данные выглядят нормально, пока оставим их*

# In[20]:


#Проанализиуем с помощью яшика с усами и графика распределения  "маркет_актив_тек_мес" и  "Покупательская_активность"
hist_with_wiskers(market_file, 'маркет_актив_тек_мес', 'покупательская_активность')


# In[21]:


def countplot_NEW (df,cal,target):
    plt.figure(figsize=(8, 5))
    sns.countplot(x=cal, data=df, palette='inferno',hue=target)
    plt.title('Частота уникальных значений признака')
    plt.xlabel('Уникальные значения')
    plt.ylabel('Количество')
    plt.xticks(rotation=25)


# In[22]:


countplot_NEW(market_file,'маркет_актив_тек_мес','покупательская_активность')


# In[23]:


#Проанализиуем с помощью яшика с усами и графика распределения  "длительность" и  "Покупательская_активность"
hist_with_wiskers(market_file, 'длительность', 'покупательская_активность')


# Признак длительность в целом распределен равномерно на всём диапазоне значений;
# 

# In[24]:


#Проанализиуем с помощью яшика с усами и графика распределения  "акционные_покупки" и  "Покупательская_активность"
hist_with_wiskers(market_file, 'акционные_покупки', 'покупательская_активность')


# Большинство покупателей не ориентируются на скидки, но есть и покупатели, которые ими интереусуется.
# Будет лучше  разделить пользователей на две части "Часто покупает по акции" и "Редко покупает по акции", превратив колонку Акционные_покупки в категоральный признак.

# In[25]:


#Посмотрим на распределение Популярная_категория
categoral_unique(market_file, 'популярная_категория') 


# Самой популярной категорией являются "товары для детей" 25,4%, затем следует "домаший текстиль" - 19,3%, после "косметика и Аксессуары" - 17,2% и другие

# In[26]:


#Проанализиуем с помощью яшика с усами и графика распределения  "средний_просмотр_категорий_за_визит" и  "Покупательская_активность"
hist_with_wiskers(market_file, 'средний_просмотр_категорий_за_визит', 'покупательская_активность')


# Признак Средний_просмотр_категорий_за_визит имеет нормальное распределение, можно сделать категориальным признаком с 6 категориями;
# 

# In[27]:


#Проанализиуем с помощью яшика с усами и графика распределения  "Неоплаченные_продукты_штук_квартал" и  "Покупательская_активность"
hist_with_wiskers(market_file, 'неоплаченные_продукты_штук_квартал', 'покупательская_активность')


# Признак Неоплаченные_продукты_штук_квартал имеет слегка скорешенное влево распределение - редко кто хранит в корзине больше 8 предметов - возможно сделаем его категориальным признаком
# 

# In[28]:


#Проанализиуем с помощью яшика с усами и графика распределения  "Ошибка_сервиса" и  "Покупательская_активность"
hist_with_wiskers(market_file, 'ошибка_сервиса', 'покупательская_активность')


# Признак Ошибка_сервиса распределен нормально - возможно сделаем его  категориальным признаком
# 

# In[29]:


#Проанализиуем с помощью яшика с усами и графика распределения  "Страниц_за_визит" и  "Покупательская_активность"
hist_with_wiskers(market_file, 'страниц_за_визит', 'покупательская_активность')


# признак страницы имеет слегка скошенное влево нормальное распределение.
# 

# ***Исследуем market_money***

# In[30]:


#Посмотрим на распределение Период
categoral_unique(market_money, 'период') 


# *данные распределены равномерно*

# In[31]:


#Посмотрим на распределение Период по выроучке
market_money_by_period = market_money.pivot_table(index='период', values=['выручка'], aggfunc='sum')
market_money_by_period


# In[32]:


#Посмотрим на распределение Выручки
pivot_pie_chart(market_money_by_period, 'выручка') 


# *Аналогично признаку "Период" данные распределены равномерно*

# In[33]:


#Проанализиуем с помощью яшика с усами и графика распределения  "выручка" 
hist_with_wiskers(market_money, 'выручка', None)


# *В выручке наблюдается скачек, вероятнее всего это выброс, посмотрим на него подробнее ниже*

# In[34]:


market_money.query('выручка>100000')


# *Уберем данный выброс*

# In[35]:


market_money=market_money[market_money['выручка']<100000]


# In[36]:


#посмотрим на изменения
hist_with_wiskers(market_money, 'выручка', None)


# *Данные выглядят сильно лучше, но теперь стало заметно значение около 0, посмотрим его подробнее ниже*

# In[37]:


market_money.query('выручка < 1')


# *Данные без выручки не интересны для исследования, удалим их*

# In[38]:


market_money = market_money[market_money['выручка'] > 0]


# In[39]:


#Проверим еще раз, что у нас получилось
hist_with_wiskers(market_money, 'выручка', None)


# *После удаления вабросов распределение "Выручнка" стало выглядит равномерным*

# ***Проанализируем market_time***

# In[40]:


market_time_by_period = market_time.pivot_table(index='период', values=['минут'], aggfunc='sum')
market_time_by_period


# Найдена опечатка в предыдцщий_месяц, заменим ее на предыдущий_месяц

# In[41]:


market_time['период']=market_time['период'].replace('предыдцщий_месяц', 'предыдущий_месяц')


# In[42]:


pivot_pie_chart(market_time_by_period, 'минут') 


# In[43]:


#Проанализиуем с помощью яшика с усами и графика распределения  "минут" 
hist_with_wiskers(market_time, 'минут', None)


# In[44]:


#Проанализиуем с помощью яшика с усами и графика распределения  "прибыль" 
hist_with_wiskers(money, 'прибыль', None)


# *По заданию "Отберите клиентов с покупательской активностью не менее трёх месяце", соберем данные*

# In[45]:


agg_dict = {'период': ['count']}
grouped = market_money.groupby('id').agg(agg_dict)
grouped.columns = ['count']

grouped = grouped[grouped['count'] == 3]
market_file = market_file[market_file['id'].isin(grouped.index)]
len(market_file)


# ***ОБЩИЙ ВЫВОД ПО БЛОКУ***

# Провели исследовательский анализ данных.
# В ходе работы были найдены опечатки в назвнаниях столбцов.Они были хорошо видны при формировани графиков. 
# Данные опечатки поправлены
# 
# Было выявлено не большое количество  аномальных значений, которые были отброшены из рассмотрения. 
# В данных оставили ставили только пользователей, совершавших покупки в течении трех последних месяцев(по требованию задания).
#   - признак "Покупательская_активность" - наблюдаетя дисбаланс.
#   - признак "Актив_6_мес" в целом распределен нормально, есть несколько слишком малых значений.Пока их не удаляли, но имеем введу.
#   - признак "Актив_тек_мес" можно сделать категориальным признаком.
#   - признак "Длительность" в целом распределен равномерно на всём диапазоне значений;
#   - признак "Акции" - будет лучше  разделить пользователей на две части "Часто покупает по акции" и "Редко покупает по акции", превратив колонку Акционные_покупки в категоральный признак.
#   - признак "Популярная_категория" -самой популярной категорией являются "товары для детей" 25,4%, затем следует "домаший текстиль" - 19,3%, после "косметика и Аксессуары" - 17,2% и другие"Популярная_категория" .
#   - признак "Средний_просмотр_категорий_за_визит" имеет нормальное распределение, можно сделать категориальным признаком с 6 категориями.
#   - признак "Неоплаченные_продукты_штук_квартал" имеет слегка скорешенное влево распределение - редко кто хранит в корзине больше 8 предметов - возможно сделаем его категориальным признаком.
#   - признак "Ошибка_сервиса" распределен нормально - возможно сделаем его  категориальным признаком.
#   - признак "Страницы" имеет слегка скошенное влево нормальное распределение.
#   - признак "Период" - даныне распределены равномерно.
#   - признак "Выручка" - Аналогично признаку "Период" данные распределены равномерно.Также были выявлены выбросы по данным >100000, и <1 (убрали их).После удаления вабросов распределение "Выручка" стало выглядит равномерным.
#   - Признак "Минут" распределен нормально.
#   - Признак "Прибыль" распределен нормально.
#  

# ***Шаг 4. Объединение таблиц***

# 4.1 Объединим таблицы market_file.csv, market_money.csv, market_time.csv. Данные о прибыли из файла money.csv при моделировании нам не понадобятся. 
# 
# 4.2 Учитывая, что данные о выручке и времени на сайте находятся в одном столбце для всех периодов. В итоговой таблице сделаем отдельный столбец для каждого периода.

# In[46]:


market_money_grouped = market_money.pivot_table(index=['id'], columns=['период'])
market_money_grouped.columns = ['выручка_предыдущий_месяц', 'выручка_препредыдущий_месяц', 'выручка_текущий_месяц']
market_money_grouped['id'] = market_money_grouped.index


market_time_grouped = market_time.pivot_table(index=['id'], columns=['период'])
market_time_grouped.columns = ['минут_предыдущий_месяц', 'минут_текущий_месяц']
market_time_grouped['id'] = market_time_grouped.index

market_full = market_file.join(market_money_grouped, on='id', lsuffix="_left", rsuffix='_выручка')
market_full = market_full.rename(columns={'id_left':'id'})
market_full = market_full.join(market_time_grouped, on='id', lsuffix="_left", rsuffix='_минут')
market_full = market_full.rename(columns={'id_left':'id'})
market_full = market_full.drop(['id_выручка', 'id_минут'], axis=1)
market_full.head(5)


# In[47]:


market_full['акционные_покупки_категория'] = market_full['акционные_покупки'].apply( lambda x: 'Часто покупает по акции' if x>= 0.5 else 'Редко покупает по акции' )
market_full = market_full.drop(['акционные_покупки'], axis=1)


# ***Вывод по блоку***
# 
# Произвели обьеденение таблиц "market_file", "market_money","market_time".
# Предварительно сгруппировали market_money в market_money_grouped и market_time в market_time_grouped и добавили названия столбцов. Обьеденили все в таблицу market_full по id

# ***Шаг 5. Корреляционный анализ***
# 
# Проведем корреляционный анализ признаков в количественной шкале в итоговой таблице для моделирования. Сделайем выводы о мультиколлинеарности и при необходимости устраним её.
# 

# In[48]:


#Для коррелияционного анализа воспользуемся матрицей phik_matrix
phik_matrix = market_full.drop('id', axis=1).phik_matrix(interval_cols=['маркет_актив_6_мес',
                                                                        'длительность', 
                                                                        'выручка_препредыдущий_месяц',
                                                                        'выручка_предыдущий_месяц', 
                                                                        'выручка_текущий_месяц'])

mask = np.triu(np.ones_like(phik_matrix, dtype=bool))

plt.figure(figsize=(15, 15))
sns.heatmap(phik_matrix, annot=True, fmt=".2f", cmap="coolwarm", mask=mask)
plt.title("Корреляционная матрица")
plt.show()


# In[49]:


market_full


# In[50]:


#Используем VIF - инструмент для обнаружения мультиколлинеарности в анализе регрессии, когда независимые переменные сильно коррелированы
interval_cols = [
    'маркет_актив_6_мес', 'маркет_актив_тек_мес', 'длительность', 
    'средний_просмотр_категорий_за_визит', 
    'неоплаченные_продукты_штук_квартал', 'ошибка_сервиса', 
    'страниц_за_визит', 'выручка_предыдущий_месяц', 
    'выручка_препредыдущий_месяц', 'выручка_текущий_месяц', 
    'минут_предыдущий_месяц', 'минут_текущий_месяц'
]

# Выбираем только числовые признаки
X = market_full[interval_cols]

# Добавляем константу
X = sm.add_constant(X)

# Вычисляем VIF для каждого признака
vif_data = pd.DataFrame()
vif_data["Variable"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

print(vif_data)


# ***Общий вывод по блоку***
# 
# Целевым признаком является Покупательская_активность.
# Список полей, которые имеют  корреляцию с ним:
#   - Маркет_актив_6_мес
#   - Средний_просмотр_категорий_за_визит
#   - Неоплаченные_продукты_штук_квартал
#   - Страниц_за_визит
#   - Выручка_препредыдущий_месяц
#   - минут_предыдущий_месяц
#   - минут_текущий_месяц
# 
# После проверки мультиколлинеальности выяснилось, что у всех признаков, кроме "выручка_предыдущий_месяц" и "выручка_текущий_месяц", мультиколлинеальности отсутсвует.
# 
# А у этих двух признаков VIF < 5, что говорит о слабой мультиколлинеальности

# ***Шаг 6. Использование пайплайнов***
# 

# Подготовим данные.
# 
# Закодируем целевой признак в значение 0 и 1. 
# 
# А так же превратим колонку Акционные_покупки в категоральный признак

# In[51]:


# Создание и использование LabelEncoder
le = LabelEncoder()
market_full['покупательская_активность'] = le.fit_transform(market_full['покупательская_активность'])
# Преобразование значений в 0 и 1
market_full['покупательская_активность'] = market_full['покупательская_активность'].astype(int)
market_full['покупательская_активность'] = le.fit_transform(market_full['покупательская_активность'])
# Проверяем результат
print(le.classes_)


# In[52]:


##market_full['акционные_покупки_категория'] = market_full['акционные_покупки']\
##.apply( lambda x: 'Часто покупает по акции' if x>= 0.5 else 'Редко покупает по акции' )
##market_full = market_full.drop(['акционные_покупки'], axis=1)


# In[53]:


market_full = market_full.set_index('id')


# In[54]:


market_full.head()


# In[55]:


X = market_full.drop(['покупательская_активность'], axis=1)
y = market_full['покупательская_активность']
X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size = TEST_SIZE, 
    random_state = RANDOM_STATE,
    stratify = y)


# In[56]:


ohe_columns = ['популярная_категория']
ord_columns = ['акционные_покупки_категория','разрешить_сообщать','тип_сервиса']


# In[57]:


num_columns = ['маркет_актив_6_мес', 'маркет_актив_тек_мес', 'маркет_актив_тек_мес', 
               'длительность', 'средний_просмотр_категорий_за_визит',
               'неоплаченные_продукты_штук_квартал', 'ошибка_сервиса', 'страниц_за_визит', 
               'выручка_предыдущий_месяц', 'выручка_препредыдущий_месяц', 'выручка_текущий_месяц', 
               'минут_предыдущий_месяц', 'минут_текущий_месяц']


# In[58]:


ohe_pipe = Pipeline(
    [
        ('simpleImputer_ohe', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),
        ('ohe', OneHotEncoder(handle_unknown='ignore', sparse=False, drop='first'))
    ]
)


# In[59]:


ord_pipe = Pipeline(
    [
        (
            'simple_imputer_ord_before',
            SimpleImputer(missing_values=np.nan, strategy='most_frequent')
        ),
        (
            'ord',
            OrdinalEncoder(categories=[
                                      ['Редко покупает по акции','Часто покупает по акции'],
                                      ['да','нет'],
                                      ['премиум','стандарт']],
                          handle_unknown='use_encoded_value',
                          unknown_value=np.nan)
        ),
        (
            'simple_imputer_ord_after',
            SimpleImputer(missing_values=np.nan, strategy='most_frequent')
        )
    ]
)


# In[60]:


data_preprocessor = ColumnTransformer(
    [('ohe', ohe_pipe, ohe_columns),
     ('ord', ord_pipe, ord_columns),
     ('num', MinMaxScaler(), num_columns)
    ], 
    remainder='passthrough'
)


# In[61]:


pipe_final = Pipeline([
    ('preprocessor', data_preprocessor),
    ('models', DecisionTreeClassifier(random_state=RANDOM_STATE))
])


# In[62]:


param_grid = [
    # словарь для модели DecisionTreeClassifier()
    {
        'models': [DecisionTreeClassifier(random_state=RANDOM_STATE)],
        'models__max_depth': range(2, 15),
        'models__max_features': range(2,7),
        'models__min_samples_split': range(2, 10),
        'preprocessor__num': [StandardScaler(), MinMaxScaler(), 'passthrough']  
    },
    
    # словарь для модели KNeighborsClassifier() 
    {
        'models': [KNeighborsClassifier()],
        'models__n_neighbors': range(2,5),
        'preprocessor__num': [StandardScaler(), MinMaxScaler(), 'passthrough']   
    },

    # словарь для модели LogisticRegression()
    {
        'models': [LogisticRegression(
            random_state=RANDOM_STATE, 
            solver='liblinear', 
            penalty='l1'
        )],
        'models__C': range(1,5),
        'preprocessor__num': [StandardScaler(), MinMaxScaler(), 'passthrough']  
    },
    # словарь для модели SVC()
    {
        'models': [SVC(random_state=RANDOM_STATE, kernel='poly',probability=True)],
        'models__degree': range(2, 5),
        'models__C': np.logspace(-2, 2, 5),
        'models__gamma': ['scale', 'auto'],
        'preprocessor__num': [StandardScaler(), MinMaxScaler(), 'passthrough']
    }
]


# In[63]:


randomized_search = RandomizedSearchCV(
    pipe_final, 
    param_grid, 
    cv=5,
    scoring='roc_auc',
    random_state=RANDOM_STATE,
    n_jobs=-1
)


# In[64]:


randomized_search.fit(X_train, y_train)


# In[65]:


print('Лучшая модель и её параметры:\n\n', randomized_search.best_estimator_)
print ('Метрика по кросс валидацинной выборке:', round(randomized_search.best_score_, 2))


# In[67]:


print(f'Метрика F1-score на тестовой выборке: {round(f1_score(y_test, y_test_pred), 2)}')


# ***ОБЩИЙ ВЫВОД ПО БЛОКУ***
# 
# На данном шаге исследования была найдена и обучена модель для определения снижения активности покупателей сервиса.
# 
# Лучшие результаты показала модель LogisticRegression(C=3, penalty='l1', random_state=46,
#                                     solver='liblinear')
# 
# Метрика roc_auc для тестовой выборки показла результат 0.89. Модель хорошо справляется с предсказанием. 
# 
# Так же метрика f1 показала 0,8
# Имеется дисбаланс в классах и все же он не помешал на получить такие высокие метрики.

# ***Шаг 7. Анализ важности признаков***

# Оценим важность признаков для лучшей модели и построим график важности с помощью метода SHAP.
# 
# - Сделаем выводы о значимости признаков:
# - какие признаки мало значимы для модели;
# - какие признаки сильнее всего влияют на целевой признак;
# - как можно использовать эти наблюдения при моделировании и принятии бизнес-решений.

# In[68]:


X_train_2 = pipe_final.named_steps['preprocessor'].fit_transform(X_train)

explainer = shap.LinearExplainer(randomized_search.best_estimator_.named_steps['models'], X_train_2)

X_test_2 = pipe_final.named_steps['preprocessor'].transform(X_test)

feature_names = pipe_final.named_steps['preprocessor'].get_feature_names_out()
X_test_2 = pd.DataFrame(X_test_2, columns=feature_names)

shap_values = explainer(X_test_2)




plt.rcParams.update({
    'font.size': 25,  
    'axes.titlesize': 25, 
    'axes.labelsize': 25, 
    'xtick.labelsize': 25,  
    'ytick.labelsize': 25,  
    'legend.fontsize': 25,  
    'figure.titlesize': 25 
})


# Подпись оси Y и название графика для первого графика
plt.ylabel('Значение SHAP')  # Подписываем ось Y
plt.title('Важность признаков')  # Название графика


shap.summary_plot(
    shap_values, 
    X_test_2, 
    plot_type="bar", 
    max_display=30, 
    plot_size=(15, 15) 
)

# Подпись оси Y и название графика для первого графика
plt.ylabel('Значение SHAP')  # Подписываем ось Y
plt.title('Важность признаков (бар-график)')  # Название графика


shap.summary_plot(
    shap_values, 
    X_test_2, 
    plot_type="dot", 
    max_display=30, 
    plot_size=(15, 15)  
)

plt.show()


# In[69]:


market_full ['акционные_покупки_категория'].value_counts()


# ***ОБЩИЙ ВЫВОД ПО БЛОКУ***
# 
# 
# Для модели важны следующие признаки это: 
# 
# - Акционные покупки
#  - Мелкая бытовая техника и электроника, 
#   - Просмотр категорий за визит
#   - Страниц за визит
#   - Минуты в предыдущий месяц.
#   
# Малозначимыми признаками оказались:
# - Популярная категория(кроме бытовой техники и техники для красоты и здоровья),
#  - Маркет_актив_тек_мес
#  - Разрешить сообщать, Тип_сервиса.
# 
# По графику beeswarm видно:
# 
# ord_акционные_покупки_категория: 
# - Высокие значения этого признака увеличивают SHAP значения и, следовательно, увеличивают вероятность того, что покупательная активность снизится. Это означает, что наличие акционных покупок больше связано со снижением покупательной активности.
# 
# num_средний_просмотр_категорий_за_визит  и num_страницы_за_визит :
#  - Низкие значения этих признаков увеличивают SHAP значения, что указывает на увеличение вероятности снижения покупательной активности. То есть чем меньше человек просмотрит категорий и посетит страниц за визит, тем выше вероятность того, что его покупательная активность снизится.
# 
# num_минут_текущий_месяц и num_минут_предыдущий_месяц: 
# - Эти признаки имеют смешанное влияние, где высокие значения могут как увеличивать, так и уменьшать вероятность целевого события, в зависимости от конкретного случая.
# 
# Остальные признаки также имеют влияние, но оно менее выражено.

# ***Шаг 8. Сегментация покупателей***

# In[70]:


y_test_proba = randomized_search.predict_proba(X_test)[:,1]
y_train_proba = randomized_search.predict_proba(X_train)[:,1]


# In[71]:


X_test_full = X_test.copy()
X_train_full = X_train.copy()
X_test_full['вероятность_снижения'] = y_test_proba
X_train_full['вероятность_снижения'] = y_train_proba
df_full = pd.concat([X_train_full, X_test_full])

money = money.set_index('id')
df_full = df_full.join(money)


# In[72]:


fig = plt.figure(figsize=(15,15))
sns.scatterplot(data=df_full, y='прибыль', x='вероятность_снижения')
plt.xlabel('Вероятность снижения активности')
plt.ylabel('Прибыль')
plt.title('Зависимость вероятности снижения активности от выручки')
plt.show()


# Получили диаграмму рассеинья Прибыли и Вероятности снижения активности покупателя. 
# 
#  - зависимости вероятности снижения активности пользователя от прибыли не видно

# In[73]:


def build_scatterplots(cat_columns):
    for cat_col in cat_columns:
        fig = plt.figure(figsize=(15,15))
        sns.scatterplot(data=df_full, y='прибыль', x='вероятность_снижения', hue=cat_col)
        plt.xlabel('Вероятность снижения активности')
        plt.ylabel('Прибыль')
        plt.title('Зависимость вероятности снижения активности от выручки')
        plt.show()


# In[74]:


plt.rcParams.update({
    'font.size': 10,  
    'axes.titlesize': 10, 
    'axes.labelsize': 10, 
    'xtick.labelsize': 10,  
    'ytick.labelsize': 10,  
    'legend.fontsize': 10,  
    'figure.titlesize':10 
})
cat_columns = list(df_full.select_dtypes(include='object').columns)
build_scatterplots(cat_columns)


# По графикам больше всего бросается в глаза это распределение Акционные_покупки_категория.
# 
# Пользователи, которые часто покупают по скидке чаще всего имеют высокую вероятность снижения активности. 
# 
# Скидки не постоянно бывают, значит и покупки этот сегмент пользователей совершает так сказать сезонно. 
# 
# И более того пользователи покупающие в основном по скидке прдставлены по всему распределению прибыли.

# In[75]:


df_full


# In[76]:


df_full['прибыль'].describe()


# In[77]:


df_full['сегмент'] = df_full.apply( lambda row: 'Исследуемый сегмент'         if row['вероятность_снижения'] > 0.8 and row['прибыль']>4         else 'Остальные пользователи'       , axis=1)


# In[78]:


categoral_unique(df_full[df_full['сегмент'] == 'Исследуемый сегмент'], 'популярная_категория')


# Видимо, дополнительные расходы, которые накладывает на семью ребёнок, заставляет людей чаще покупать товары по скидкам.

# In[79]:


categoral_unique(df_full[df_full['сегмент'] == 'Исследуемый сегмент'], 'тип_сервиса')


# Люди, которые совершают покупки по скидкам стараются экономить на всём, в том числе и на подписках

# In[80]:


hist_with_wiskers(df_full, 'страниц_за_визит', 'сегмент')


# Видно по пользователи по скидке просматривают меньше страниц. Похоже на то что эти пользователи не ищут и выбирают, а покупают целенаправленно.

# In[81]:


countplot_NEW(df_full, 'средний_просмотр_категорий_за_визит', 'сегмент')


# В среднее кол-во просматриваемых категорий не различается.

# In[82]:


countplot_NEW(df_full, 'неоплаченные_продукты_штук_квартал', 'сегмент')


# In[83]:


hist_with_wiskers(df_full, 'маркет_актив_6_мес', 'сегмент')


# In[84]:


countplot_NEW(df_full,'маркет_актив_тек_мес','сегмент')


# Пользователи из иследуемого сегмента в среднем оставляют неоплаченными в корзине больше товаров.

# Скорее всего, магазин не предоставляет пользователям каких либо дополнительных источников получения информации о скидках

# ***ОБЩИЙ ВЫВОД ПО ПРОЕКТУ***
# 
# Задачей данного иссследования было построение модели, которая бы предсказала уменьшение покупательской активности пользователей интернет-магазина "В один клик".
# Другой задачей было исследование выбранного сегмента пользователей.
# 
# В рамках исследования были проделаны следующие шаги.
# 
# **Загрузка данных**
# 
# Исходными данными для исследования были четыре отдельных csv файла:
# 
# - с данными о поведении покупателя на сайте, коммуникации с ним и его продуктовом поведении.
# - с данными о выручке, которую получает магазин с покупателя за период
# - с данными о времени (в минутах), которое покупатель провёл на сайте в течение периода
# 
# данными о среднемесячной прибыли покупателя за последние 3 месяца: какую прибыль получает магазин от продаж каждому покупателю 
# 
# Данные были загружены в датафреймы библиотеки pandas.
# 
# **Предобработка данных**
# 
# Исходные данные оказались хорошего качества, но тем не менее они требовали проведения некоторых работ перед началом анализа. Во-первых стандартизация имен признаков. Наименования были оставлены на кириллице, но пробелы были заменены на землю.
# Во-вторых обработка пропусков. Пропусков в данных не было найдено.
# 
# **Исследовательский анализ данных**
# 
# На этом этапе были построены диаграммы распределения всех признаков. С помощью графиков были обнаружены и исправлены опечатки в категоральных признаках. Так же отметим, что график распределения признака Акционные_покупки явно выделял две группы пользователей поэтому признак был превращен в категориальный.
# 
# **Корреляционный анализ данных**
# 
# Целевым признаком является Покупательская_активность и вот список полей, которые имеют хоть корреляцию с ним: 
# 
# - Маркет_актив_6_мес
#  - Акционные_покупки
#  - Средний_просмотр_категорий_за_визит
#  - Неоплаченные_продукты_штук_квартал
#  - Страниц_за_визит
#  - Выручка_препредыдущий_месяц
#  - минут_предыдущий_месяц
#  - минут_текущий_месяц.
#  
# Сильной корреляции между другими признаками не было выявлено поэтому все остальные признаки были оставленны в датасете.
# 
# Проверка на мультиколлинеарность не выявила сильных зависимостей между признаками.
# 
# **Объеденение таблиц**
# 
# Датасеты market_file, market_money, market_time были объеденены в один датафрейм.
# 
# **Обучение модели**
# 
# С использованием пайпланов из библиотеки sklearn были обучены модели 
#  - KNeighborsClassifier()
#  - DecisionTreeClassifier()
#  - LogisticRegression() 
#  - SVC(). 
#  
#  При обучении моделей средстави пайплана преебирались некоторое количество гиперпараметров моделей с целью нахождения модели, которая даст лучший результат.
# По результатам обучения лучшей стала модель  LogisticRegression(C=3, penalty='l1', random_state=46, solver='liblinear')
# 
# Метрика roc_auc для тестовой выборки показла результат 0.89. Модель хорошо справляется с предсказанием.
# 
# Так же метрика f1 показала 0,84 Имеется дисбаланс в классах и все же он не помешал на получить такие высокие метрики.
# 
# **Анализ важности признаков**
# 
# Самыми важными оказались признаки связанные с аукционными товарами и с количеством времени, которое покупатель проводит на меркет плейсе. А наименее важными, категории товаров, котоыре интересовали пользователи и коммуникации с пользователем.
# Кажется что все эти маркетинговые завлекаловки и спам на почту не помогают маркетплейсу. Важно лишь заставить пользователя проводить на сайте как можно больше времени и продажи выростут.
# 
# **Сегментация пользователей**
# 
# Для анализа пользователей был выбран сегмент пользователей часто покупающих по скидке и в то же время имеющийх высокую вероятность снижения покупательской активности. Мое предложение по увеличение актвности этого сегмента покупателей это таргетированная настройка главной страницы маркетплейса. А так же более частое обновление скидок на главной странице. Пользователь не хочет ходить по страницам, он хочет видеть все предложения в одном месте.
